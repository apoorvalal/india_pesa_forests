{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from selenium import webdriver\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_driver_loc = 'C:/Users/nikhi/Downloads/chromedriver'\n",
    "driver = webdriver.Chrome(chrome_driver_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = pd.read_csv('C:/Users/nikhi/Dropbox/Jharkhand Minerals/Lessee Profile Links.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RML - rptAccordian_ctl00_tblView\n",
    "#AUCTION - rptAccordian_ctl00_tblView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 37\n",
    "link = links['detail_link'][i]\n",
    "driver.get(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns(df):\n",
    "    cols = list(df.columns)\n",
    "    for i in range(len(cols)):\n",
    "        c = cols[i]\n",
    "        try :\n",
    "            temp = int(c[c.rfind('.')+1:])\n",
    "            cols[i] = c[:c.rfind('.')]\n",
    "        except:\n",
    "            continue\n",
    "    return pd.DataFrame([cols]+df.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_update_status(link):\n",
    "    r = requests.get(link)\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    tabs = soup.find_all(\"div\",{\"id\":\"myTab\"})\n",
    "    images = []\n",
    "    for tab in tabs:\n",
    "        images.extend(tab.find_all('img'))\n",
    "    tags = {}\n",
    "    count = 1\n",
    "    for image in images:\n",
    "        if 'oknew' in str(image):\n",
    "            tags.update({count:'Updated'})\n",
    "        if 'cancelnew' in str(image):\n",
    "            tags.update({count:'Not Updated'})\n",
    "        count+=1\n",
    "    return {'updateStatus':tags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LESSEE DETAILS\n",
    "def get_lessee_details(link):\n",
    "    linkId = 'LesseeProfile'\n",
    "    tableId = 'grdLProfile'\n",
    "    r = requests.get(link.replace('LesseeProfile',linkId))\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    pre = ''\n",
    "    details = {}\n",
    "    try:\n",
    "        df = pd.read_html(str(soup.find(\"table\",{\"id\":tableId})))[1]\n",
    "    except:\n",
    "        return details\n",
    "    for l in df.values:\n",
    "        if len(set([x for x in l if x==x])) == 1:\n",
    "            pre = l[0] +' '\n",
    "            continue\n",
    "        i=1\n",
    "        last_detail = 'abcdef'\n",
    "        while i<len(l):\n",
    "            if l[i-1]==last_detail:\n",
    "                i+=1\n",
    "                continue\n",
    "            details.update({pre+str(l[i-1]):l[i]})\n",
    "            last_detail = l[i]\n",
    "            if l[i] == 'Contact Person Name':\n",
    "                break\n",
    "            i+=2\n",
    "    return {linkId:details}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEASE INFO\n",
    "def get_lease_info(link):\n",
    "    linkId = 'LesseeInfo'\n",
    "    tableId = 'grdLeaseInfo'\n",
    "    pre = ''\n",
    "    details = {}\n",
    "    r = requests.get(link.replace('LesseeProfile',linkId))\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    try:\n",
    "        df = pd.read_html(str(soup.find(\"table\",{'id':tableId})))[1]\n",
    "    except:\n",
    "        return details\n",
    "    for l in df.values:\n",
    "        if len(set([x for x in l if x==x])) == 1:\n",
    "            pre = l[0] +' '\n",
    "            continue\n",
    "        i=1\n",
    "        if l[0] == 'Non Forest Area (HA)':\n",
    "            pre = pre+l[0]+' '\n",
    "            i=2\n",
    "        last_detail = 'abcdef'\n",
    "        while i<len(l):\n",
    "            if l[i-1]==last_detail:\n",
    "                i+=1\n",
    "                continue\n",
    "            details.update({pre+str(l[i-1]):l[i]})\n",
    "            last_detail = l[i]\n",
    "            i+=2\n",
    "    return {linkId:details}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOREST CLEARANCE\n",
    "def get_forest_clearance(link):\n",
    "    linkId = 'ForestClearance'\n",
    "    tableId = 'grdForClear'\n",
    "    pre = ''\n",
    "    details = {}\n",
    "    r = requests.get(link.replace('LesseeProfile',linkId))\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    try:\n",
    "        df = pd.read_html(str(soup.find(\"table\",{\"id\":tableId})))[1]\n",
    "    except:\n",
    "        return details\n",
    "    df = df['Forest Clearance']\n",
    "    df = remove_columns(df)\n",
    "    for l in df.values:\n",
    "        if len(set([x for x in l if x==x])) == 1:\n",
    "            pre = l[0] +' '\n",
    "            continue\n",
    "        i=1\n",
    "        if l[0] == 'Stage 1 Clearance':\n",
    "            pre = pre+l[0]+' '\n",
    "            i=3\n",
    "        elif l[0] == 'Stage 2 Clearance':\n",
    "            pre = 'Non-Forest Land Details '+l[0]+' '\n",
    "            i=3        \n",
    "        last_detail = 'abcdef'\n",
    "        while i<len(l):\n",
    "            if l[i-1]==last_detail:\n",
    "                i+=1\n",
    "                continue\n",
    "            details.update({pre+str(l[i-1]):l[i]})\n",
    "            last_detail = l[i]\n",
    "            i+=2\n",
    "    return {linkId:details}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENVIRONMENT CLEARANCE\n",
    "def get_env_clearance(link):\n",
    "    linkId = 'EnvClearance'\n",
    "    tableId = 'grdEnv'\n",
    "    r = requests.get(link.replace('LesseeProfile',linkId))\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    details = {}\n",
    "    try:\n",
    "        df = pd.read_html(str(soup.find(\"table\",{\"id\":tableId})))[0]\n",
    "    except:\n",
    "        return details\n",
    "    for i in df.index:\n",
    "        for c in df.columns:\n",
    "            details.update({c+' '+str(i+1):df[c][i]})\n",
    "    return {linkId:details}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POLLUTION CLEARANCE\n",
    "def get_pol_clearance(link):\n",
    "    linkId = 'PCBClearance'\n",
    "    tableId = 'grdOspcbML'\n",
    "    pre = ''\n",
    "    details = {}\n",
    "    r = requests.get(link.replace('LesseeProfile',linkId))\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    try:\n",
    "        dfs = pd.read_html(str(soup.find(\"table\",{\"id\":tableId})))\n",
    "    except:\n",
    "        return details\n",
    "    for j in range(1,len(dfs)):\n",
    "        df = remove_columns(dfs[j])\n",
    "        for l in df.values:\n",
    "            if len(set([x for x in l if x==x])) == 1:\n",
    "                pre = l[0] +' ' + str(j)+' '\n",
    "                continue\n",
    "            i=1\n",
    "            last_detail = 'abcdef'\n",
    "            while i<len(l):\n",
    "                if l[i-1]==last_detail:\n",
    "                    i+=1\n",
    "                    continue\n",
    "                details.update({pre+str(l[i-1]):l[i]})\n",
    "                last_detail = l[i]\n",
    "                i+=2\n",
    "    return {linkId:details}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SURVEY\n",
    "def get_survey_details(link):\n",
    "    linkId = 'Survey'\n",
    "    tableId = 'grdSurveyML'\n",
    "    pre = ''\n",
    "    details = {}\n",
    "    r = requests.get(link.replace('LesseeProfile',linkId))\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    try:\n",
    "        df = pd.read_html(str(soup.find(\"table\",{\"id\":tableId})))[1]\n",
    "    except:\n",
    "        return details\n",
    "    df = remove_columns(df)\n",
    "    for l in df.values:\n",
    "        if len(set([x for x in l if x==x])) == 1:\n",
    "            pre = l[0] +' '\n",
    "            continue\n",
    "        i=1\n",
    "        last_detail = 'abcdef'\n",
    "        while i<len(l):\n",
    "            if l[i-1]==last_detail:\n",
    "                i+=1\n",
    "                continue\n",
    "            details.update({pre+str(l[i-1]):l[i]})\n",
    "            last_detail = l[i]\n",
    "            i+=2\n",
    "    return {linkId:details}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MINING PLAN\n",
    "def get_mining_plan_details(link):\n",
    "    linkId = 'MiningPlan'\n",
    "    tableId = 'tbML'\n",
    "    pre = 'Mining Plan '\n",
    "    details = {}\n",
    "    r = requests.get(link.replace('LesseeProfile',linkId))\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    try:\n",
    "        df = pd.read_html(str(soup.find(\"table\",{\"id\":tableId})))[0].iloc[1:5,1:]\n",
    "    except:\n",
    "        return details\n",
    "    for l in df.values:\n",
    "        if len(set([x for x in l if x==x])) == 1:\n",
    "            pre = l[0] +' '\n",
    "            continue\n",
    "        i=1\n",
    "        last_detail = 'abcdef'\n",
    "        while i<len(l):\n",
    "            if l[i-1]==last_detail:\n",
    "                i+=1\n",
    "                continue\n",
    "            if pre in l[i-1]:\n",
    "                details.update({l[i-1]:l[i]})\n",
    "            else:\n",
    "                details.update({pre+str(l[i-1]):l[i]})\n",
    "            last_detail = l[i]\n",
    "            i+=2\n",
    "    return {linkId+'Details':details}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MINING PLAN 2\n",
    "def get_mining_plan_grid_details(link):\n",
    "    linkId = 'MiningPlan'\n",
    "    tableId = 'grdMiningPlan'\n",
    "    r = requests.get(link.replace('LesseeProfile',linkId))\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    details = {}\n",
    "    try:\n",
    "        df = pd.read_html(str(soup.find(\"table\",{\"id\":tableId})))[0]\n",
    "    except:\n",
    "        return details\n",
    "    for i in df.index:\n",
    "        for c in df.columns:\n",
    "            details.update({c+' '+str(i+1):df[c][i]})\n",
    "    return {linkId:details}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRANT/EXECUTION\n",
    "def get_grant_details(link):\n",
    "    linkId = 'Grant'\n",
    "    tableId = 'grLGrant'\n",
    "    pre = ''\n",
    "    details = {}\n",
    "    r = requests.get(link.replace('LesseeProfile',linkId))\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    try:\n",
    "        df = pd.read_html(str(soup.find(\"table\",{\"id\":tableId})))[1]\n",
    "    except:\n",
    "        return details\n",
    "    df = remove_columns(df)\n",
    "    for l in df.values:\n",
    "        if len(set([x for x in l if x==x])) == 1:\n",
    "            pre = l[0] +' '\n",
    "            continue\n",
    "        i=1\n",
    "        last_detail = 'abcdef'\n",
    "        while i<len(l):\n",
    "            if l[i-1]==last_detail:\n",
    "                i+=1\n",
    "                continue\n",
    "            details.update({pre+str(l[i-1]):l[i]})\n",
    "            last_detail = l[i]\n",
    "            i+=2\n",
    "    return {linkId:details}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SURFACE RIGHT GRANT/POSSESSION\n",
    "def get_surface_right_details(link):\n",
    "    linkId = 'SurfaceRightCL'\n",
    "    tableId = 'grdSurface'\n",
    "    pre = ''\n",
    "    details = {}\n",
    "    r = requests.get(link.replace('LesseeProfile',linkId))\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    try:\n",
    "        dfs = pd.read_html(str(soup.find(\"table\",{\"id\":tableId})))\n",
    "    except:\n",
    "        return details\n",
    "    for j in range(1,3):\n",
    "        df = remove_columns(dfs[j])\n",
    "        for l in df.values:\n",
    "            if len(set([x for x in l if x==x])) == 1:\n",
    "                pre = l[0] +' '\n",
    "                continue\n",
    "            i=1\n",
    "            last_detail = 'abcdef'\n",
    "            while i<len(l):\n",
    "                if l[i-1]==last_detail:\n",
    "                    i+=1\n",
    "                    continue\n",
    "                details.update({pre+str(l[i-1]):l[i]})\n",
    "                last_detail = l[i]\n",
    "                if '(HA)' in l[i-1]:\n",
    "                    break\n",
    "                i+=2\n",
    "    return {linkId:details}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCC\n",
    "def get_dcc_details(link):\n",
    "    linkId = 'DCC'\n",
    "    tableId = 'grdDCC'\n",
    "    details = {}\n",
    "    r = requests.get(link.replace('LesseeProfile',linkId))\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    try:\n",
    "        df = pd.read_html(str(soup.find(\"table\",{\"id\":tableId})))[0]\n",
    "    except:\n",
    "        return details\n",
    "    for i in df.index:\n",
    "        for c in df.columns:\n",
    "            details.update({c+' '+str(i+1):df[c][i]})\n",
    "    return {linkId:details}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainDict = {}\n",
    "for i in links[links['ML/RML']=='ML'].index:\n",
    "    try:\n",
    "        link = links['detail_link'][i]\n",
    "        code = links['Lessee Code'][i]\n",
    "        detailsDict = {}\n",
    "        detailsDict.update(get_lessee_details(link))\n",
    "        detailsDict.update(get_lease_info(link))\n",
    "        detailsDict.update(get_forest_clearance(link))\n",
    "        detailsDict.update(get_env_clearance(link))\n",
    "        detailsDict.update(get_pol_clearance(link))\n",
    "        detailsDict.update(get_survey_details(link))\n",
    "        detailsDict.update(get_mining_plan_details(link))\n",
    "        detailsDict.update(get_mining_plan_grid_details(link))\n",
    "        detailsDict.update(get_grant_details(link))\n",
    "        detailsDict.update(get_surface_right_details(link))\n",
    "        detailsDict.update(get_dcc_details(link))\n",
    "        detailsDict.update(get_update_status(link))\n",
    "        mainDict.update({int(code):detailsDict})\n",
    "    except:\n",
    "        continue\n",
    "np.save(\"C:/Users/nikhi/Dropbox/Jharkhand Minerals/newDetailsDict.npy\",mainDict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
